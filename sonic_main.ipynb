{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, gc, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Core data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric  # optional\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Audio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "import music21\n",
    "\n",
    "# UI\n",
    "import streamlit as st\n",
    "\n",
    "# pydub â†” ffmpeg (use system/conda ffmpeg if available)\n",
    "AudioSegment.converter = which(\"ffmpeg\") or AudioSegment.converter\n",
    "\n",
    "# Quiet a few noisy-but-harmless warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Triton.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weight_norm is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated.*\")\n",
    "\n",
    "# Version sanity checks (comment out if you prefer flexibility)\n",
    "assert torch.__version__.startswith(\"2.1.0\"), f\"Torch pin expected 2.1.0, got {torch.__version__}\"\n",
    "assert transformers.__version__.startswith(\"4.37.\"), f\"Transformers pin expected 4.37.x, got {transformers.__version__}\"\n",
    "import numpy  # keep separate to check the exact package version below\n",
    "assert numpy.__version__ == \"1.26.4\", f\"Numpy pin expected 1.26.4, got {numpy.__version__}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509c60f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoEmotions 1: (70000, 37)\n",
      "                                                 text       id       author  \\\n",
      "0                                    That game hurt.  eew5j0j        Brdd9   \n",
      "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk  TheGreen888   \n",
      "\n",
      "          subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0               nrl  t3_ajis4z  t1_eew18eq  1.548381e+09         1   \n",
      "1  unpopularopinion  t3_ai4q37   t3_ai4q37  1.548084e+09        37   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         0      0   \n",
      "1                  True           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        1         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "GoEmotions 2: (70000, 37)\n",
      "                              text       id             author  \\\n",
      "0                     We can hope  ee3o3ko      darkenseyreth   \n",
      "1  Shhh don't give them the idea!  eebl3z7  BoinkBoinkEtAliae   \n",
      "\n",
      "         subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0   EdmontonOilers  t3_ag4r9j  t1_ee3mhad  1.547529e+09        62   \n",
      "1  MurderedByWords  t3_ah3o76  t1_eeb68lo  1.547777e+09        76   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         1      0   \n",
      "1                 False           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        0         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "GoEmotions 3: (71225, 37)\n",
      "                                                 text       id        author  \\\n",
      "0  Worst ending ever! I won't spoil it but this o...  eee021b    tribaltrak   \n",
      "1                     Happy cake day u/sneakpeekbot!  ed00f1z  InfernicFuse   \n",
      "\n",
      "     subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0       movies  t3_agxbsx   t3_agxbsx  1.547847e+09        42   \n",
      "1  danganronpa  t3_abajdo  t1_eczoocj  1.546328e+09        24   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         0      0   \n",
      "1                 False           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        0         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "Column consistency check: True\n",
      "Columns: ['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      " Combined GoEmotions shape: (211225, 37)\n",
      "                                                     text  love  nervousness  \\\n",
      "68565   Levels mate. Absolutely obsessed with our domi...     0            0   \n",
      "34818   [NAME] will get booed at home after a golden s...     0            0   \n",
      "196471  ITT: People saying \"I'm doing to get downvoted...     0            0   \n",
      "\n",
      "        optimism  pride  realization  relief  remorse  sadness  surprise  \\\n",
      "68565          0      0            0       0        0        1         0   \n",
      "34818          0      0            0       0        0        0         0   \n",
      "196471         0      0            0       0        0        0         0   \n",
      "\n",
      "        neutral  \n",
      "68565         0  \n",
      "34818         0  \n",
      "196471        0  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/raw\"\n",
    "\n",
    "# Load GoEmotions parts\n",
    "geo1 = pd.read_csv(os.path.join(data_path, \"goemotions_1.csv\"))\n",
    "geo2 = pd.read_csv(os.path.join(data_path, \"goemotions_2.csv\"))\n",
    "geo3 = pd.read_csv(os.path.join(data_path, \"goemotions_3.csv\"))\n",
    "\n",
    "# Preview each\n",
    "print(f\"GoEmotions 1: {geo1.shape}\\n\", geo1.head(2), \"\\n\")\n",
    "print(f\"GoEmotions 2: {geo2.shape}\\n\", geo2.head(2), \"\\n\")\n",
    "print(f\"GoEmotions 3: {geo3.shape}\\n\", geo3.head(2), \"\\n\")\n",
    "\n",
    "# Confirm columns are the same\n",
    "print(\"Column consistency check:\", geo1.columns.equals(geo2.columns) and geo1.columns.equals(geo3.columns))\n",
    "print(\"Columns:\", geo1.columns.tolist())\n",
    "\n",
    "# Combine all three\n",
    "goemotions_df = pd.concat([geo1, geo2, geo3], ignore_index=True)\n",
    "print(\" Combined GoEmotions shape:\", goemotions_df.shape)\n",
    "print(goemotions_df[['text'] + list(goemotions_df.columns[-10:])].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f428df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned GoEmotions sample:\n",
      "                                                     text      label\n",
      "35665            out of curiosity, What are you watching?  curiosity\n",
      "107764  [NAME] has continued to show how selfish she i...   surprise\n",
      "193067                                          Source? ðŸ˜‚  curiosity\n",
      "125576  This is so true!! I'm vaping mango right now! ...  annoyance\n",
      "172973                 The best ever you are the [NAME]!!  annoyance\n",
      "Shape: (197283, 2)\n",
      "âœ… Cleaned Dreaddit sample:\n",
      "                                                   text        label\n",
      "2471  I live in BC and im gonna be homeless soon, I'...  no_distress\n",
      "3482  I then confronted my parents and finally the w...  no_distress\n",
      "2902  I feel like a completely different person. My ...     distress\n",
      "2720  I'm in need of quick assistance to make a purc...  no_distress\n",
      "1861  I've stayed up at night hearing them quietly t...     distress\n",
      "Shape: (3553, 2)\n"
     ]
    }
   ],
   "source": [
    "# --- GoEmotions cleanup ---\n",
    "# Remove unclear examples\n",
    "goemotions_df = goemotions_df[goemotions_df[\"example_very_unclear\"] == False].reset_index(drop=True)\n",
    "\n",
    "# Emotion columns are after the first 10 metadata columns\n",
    "emotion_columns = goemotions_df.columns[10:]\n",
    "\n",
    "# Keep rows with at least one emotion tag\n",
    "goemotions_df = goemotions_df[goemotions_df[emotion_columns].sum(axis=1) > 0]\n",
    "\n",
    "# Pick the first tagged emotion as a single label\n",
    "goemotions_df[\"label\"] = goemotions_df[emotion_columns].idxmax(axis=1)\n",
    "\n",
    "# Keep only text and label\n",
    "goemotions_clean = goemotions_df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "# Quick peek\n",
    "print(\"Cleaned GoEmotions sample:\")\n",
    "print(goemotions_clean.sample(5))\n",
    "print(\"Shape:\", goemotions_clean.shape)\n",
    "\n",
    "# Save\n",
    "goemotions_clean.to_csv(\"data/raw/goemotions_df.csv\", index=False)\n",
    "\n",
    "# --- Dreaddit cleanup ---\n",
    "df_train = pd.read_csv(\"data/raw/dreaddit-train.csv\")\n",
    "df_test = pd.read_csv(\"data/raw/dreaddit-test.csv\")\n",
    "\n",
    "# Merge and keep only text/label\n",
    "dreaddit_full = pd.concat([df_train, df_test], ignore_index=True)[[\"text\", \"label\"]]\n",
    "dreaddit_full.dropna(subset=[\"text\", \"label\"], inplace=True)\n",
    "\n",
    "# Map numeric to readable labels\n",
    "dreaddit_full[\"label\"] = dreaddit_full[\"label\"].map({1: \"distress\", 0: \"no_distress\"})\n",
    "\n",
    "# Quick peek\n",
    "print(\"Cleaned Dreaddit sample:\")\n",
    "print(dreaddit_full.sample(5))\n",
    "print(\"Shape:\", dreaddit_full.shape)\n",
    "\n",
    "# Save\n",
    "dreaddit_full.to_csv(\"data/raw/dreaddit_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6da5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "goemotions_df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "goemotions_df['label'] = goemotions_df['label'].astype('category')\n",
    "goemotions_df['label_id'] = goemotions_df['label'].cat.codes\n",
    "\n",
    "label2id = dict(enumerate(goemotions_df['label'].cat.categories))\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "goemotions_dataset = Dataset.from_pandas(\n",
    "    goemotions_df[['text', 'label_id']].rename(columns={'label_id': 'label'})\n",
    ")\n",
    "\n",
    "# Model + tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=label2id,\n",
    "    label2id=id2label\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"Using device:\", model.device)\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = goemotions_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_goemotions_v2\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_goemotions_v2\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model + tokenizer\n",
    "save_path = \"./saved_models/goemotions_distilbert_v3\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e66d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./saved_models/goemotions_distilbert_v3\"\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f76d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep Dreaddit dataset\n",
    "dreaddit_df = pd.read_csv(\"data/raw/dreaddit_df.csv\")[['text', 'label']].dropna()\n",
    "dreaddit_df[\"label\"] = dreaddit_df[\"label\"].map({\"no_distress\": 0, \"distress\": 1})\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dreaddit_dataset = Dataset.from_pandas(dreaddit_df)\n",
    "\n",
    "# Tokenizer\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer_dreaddit = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer_dreaddit(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dreaddit = dreaddit_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dreaddit = tokenized_dreaddit.train_test_split(test_size=0.1)\n",
    "\n",
    "# Binary classifier\n",
    "model_dreaddit = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# Training setup\n",
    "training_args_dreaddit = TrainingArguments(\n",
    "    output_dir=\"./results_dreaddit\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_dreaddit\"\n",
    ")\n",
    "\n",
    "trainer_dreaddit = Trainer(\n",
    "    model=model_dreaddit,\n",
    "    args=training_args_dreaddit,\n",
    "    train_dataset=tokenized_dreaddit[\"train\"],\n",
    "    eval_dataset=tokenized_dreaddit[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer_dreaddit\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer_dreaddit.train()\n",
    "\n",
    "# Save model + tokenizer\n",
    "save_path = \"saved_models/distress_classifier\"\n",
    "trainer_dreaddit.save_model(save_path)\n",
    "tokenizer_dreaddit.save_pretrained(save_path)\n",
    "print(f\"Distress model saved to: {save_path}\")\n",
    "\n",
    "# Evaluate\n",
    "metrics_dreaddit = trainer_dreaddit.evaluate()\n",
    "print(\"Distress Model Evaluation Results:\")\n",
    "for k, v in metrics_dreaddit.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Quick inference demo\n",
    "sample_text = [\"I'm feeling overwhelmed and anxious.\"]\n",
    "inputs = tokenizer_dreaddit(sample_text, return_tensors=\"pt\").to(model_dreaddit.device)\n",
    "outputs = model_dreaddit(**inputs)\n",
    "pred = torch.argmax(outputs.logits, dim=1)\n",
    "print(f\"Predicted label: {'distress' if pred.item() == 1 else 'no_distress'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3ac8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model + tokenizer\n",
    "save_path = \"./saved_models/goemotions_distilbert_v3\"\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path).to(\"cuda\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Load dataset and map labels to ids\n",
    "goemotions_df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "goemotions_df[\"label\"] = goemotions_df[\"label\"].astype(\"category\")\n",
    "goemotions_df[\"label_id\"] = goemotions_df[\"label\"].cat.codes\n",
    "\n",
    "goemotions_dataset = Dataset.from_pandas(\n",
    "    goemotions_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"})\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    return loaded_tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized = goemotions_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.train_test_split(test_size=0.1)\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# Eval setup\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"./logs_eval\"\n",
    ")\n",
    "\n",
    "trainer_eval = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=loaded_tokenizer\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = trainer_eval.evaluate()\n",
    "\n",
    "# Report\n",
    "print(\"GoEmotions Model Evaluation (Reloaded):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d04ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original rows: 197283, unique by text: 57080\n",
      "Using half of unique set: 28537 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shubham Mahale\\AppData\\Local\\Temp\\ipykernel_25780\\1840795712.py:46: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(frac=0.5, random_state=SEED) if len(g) > 1 else g)\n",
      "d:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d559fa72654e498603929dc5d8e3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190bf32c480c4344bc9728698499b468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce GTX 1650 Ti\n",
      "Starting trainingâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97b8ce0b0a9418389170f078c87b926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4015 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2992, 'learning_rate': 4.149377593360996e-06, 'epoch': 0.06}\n",
      "{'loss': 3.2958, 'learning_rate': 8.298755186721992e-06, 'epoch': 0.12}\n",
      "{'loss': 3.3018, 'learning_rate': 1.2448132780082988e-05, 'epoch': 0.19}\n",
      "{'loss': 3.2682, 'learning_rate': 1.6597510373443984e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1788, 'learning_rate': 1.9999719360404577e-05, 'epoch': 0.31}\n",
      "{'loss': 2.8435, 'learning_rate': 1.998794179447606e-05, 'epoch': 0.37}\n",
      "{'loss': 2.495, 'learning_rate': 1.9958864102458794e-05, 'epoch': 0.44}\n",
      "{'loss': 2.4618, 'learning_rate': 1.9912536649858366e-05, 'epoch': 0.5}\n",
      "{'loss': 2.3064, 'learning_rate': 1.984903968051158e-05, 'epoch': 0.56}\n",
      "{'loss': 2.2653, 'learning_rate': 1.9768483177596008e-05, 'epoch': 0.62}\n",
      "{'loss': 2.1955, 'learning_rate': 1.9671006673127994e-05, 'epoch': 0.68}\n",
      "{'loss': 2.0541, 'learning_rate': 1.955677900627908e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0724, 'learning_rate': 1.9425998030929474e-05, 'epoch': 0.81}\n",
      "{'loss': 2.1001, 'learning_rate': 1.9278890272965097e-05, 'epoch': 0.87}\n",
      "{'loss': 1.9927, 'learning_rate': 1.911571053791183e-05, 'epoch': 0.93}\n",
      "{'loss': 2.0926, 'learning_rate': 1.893674146958655e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb848ad97384a8ca7a2e75e53dae890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0359244346618652, 'eval_accuracy': 0.3342676944639103, 'eval_f1_weighted': 0.29151657677233767, 'eval_f1_macro': 0.31101580615351104, 'eval_runtime': 74.4456, 'eval_samples_per_second': 38.337, 'eval_steps_per_second': 0.604, 'epoch': 1.0}\n",
      "{'loss': 1.8118, 'learning_rate': 1.8742293060529445e-05, 'epoch': 1.06}\n",
      "{'loss': 1.8834, 'learning_rate': 1.8532702115065525e-05, 'epoch': 1.12}\n",
      "{'loss': 1.793, 'learning_rate': 1.8308331665925486e-05, 'epoch': 1.18}\n",
      "{'loss': 1.8543, 'learning_rate': 1.8069570345436236e-05, 'epoch': 1.25}\n",
      "{'loss': 1.8725, 'learning_rate': 1.781683171237041e-05, 'epoch': 1.31}\n",
      "{'loss': 1.8837, 'learning_rate': 1.7550553535620684e-05, 'epoch': 1.37}\n",
      "{'loss': 1.7438, 'learning_rate': 1.7271197035939767e-05, 'epoch': 1.43}\n",
      "{'loss': 1.9548, 'learning_rate': 1.697924608705937e-05, 'epoch': 1.49}\n",
      "{'loss': 1.7632, 'learning_rate': 1.6675206377571925e-05, 'epoch': 1.56}\n",
      "{'loss': 1.7338, 'learning_rate': 1.6359604535026768e-05, 'epoch': 1.62}\n",
      "{'loss': 1.8384, 'learning_rate': 1.6032987213757917e-05, 'epoch': 1.68}\n",
      "{'loss': 1.9424, 'learning_rate': 1.5695920148023444e-05, 'epoch': 1.74}\n",
      "{'loss': 1.8109, 'learning_rate': 1.534898717209646e-05, 'epoch': 1.81}\n",
      "{'loss': 1.757, 'learning_rate': 1.4992789209005105e-05, 'epoch': 1.87}\n",
      "{'loss': 1.8583, 'learning_rate': 1.4627943229672992e-05, 'epoch': 1.93}\n",
      "{'loss': 1.7293, 'learning_rate': 1.4255081184263163e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7bd033dfcf4c1cb18cf3c54dcbf404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9112491607666016, 'eval_accuracy': 0.38507358093903293, 'eval_f1_weighted': 0.36198189782394063, 'eval_f1_macro': 0.36123166984131244, 'eval_runtime': 92.9724, 'eval_samples_per_second': 30.697, 'eval_steps_per_second': 0.484, 'epoch': 2.0}\n",
      "{'loss': 1.518, 'learning_rate': 1.3874848907576422e-05, 'epoch': 2.05}\n",
      "{'loss': 1.5943, 'learning_rate': 1.3487905000400103e-05, 'epoch': 2.12}\n",
      "{'loss': 1.5887, 'learning_rate': 1.3094919688744852e-05, 'epoch': 2.18}\n",
      "{'loss': 1.5181, 'learning_rate': 1.2696573662945321e-05, 'epoch': 2.24}\n",
      "{'loss': 1.5666, 'learning_rate': 1.2293556898635646e-05, 'epoch': 2.3}\n",
      "{'loss': 1.5224, 'learning_rate': 1.1886567461641791e-05, 'epoch': 2.37}\n",
      "{'loss': 1.5225, 'learning_rate': 1.1476310298860886e-05, 'epoch': 2.43}\n",
      "{'loss': 1.6085, 'learning_rate': 1.1063496017221868e-05, 'epoch': 2.49}\n",
      "{'loss': 1.5728, 'learning_rate': 1.064883965284234e-05, 'epoch': 2.55}\n",
      "{'loss': 1.5631, 'learning_rate': 1.0233059432513654e-05, 'epoch': 2.62}\n",
      "{'loss': 1.5546, 'learning_rate': 9.816875529659416e-06, 'epoch': 2.68}\n",
      "{'loss': 1.5134, 'learning_rate': 9.401008816922222e-06, 'epoch': 2.74}\n",
      "{'loss': 1.4927, 'learning_rate': 8.986179617539289e-06, 'epoch': 2.8}\n",
      "{'loss': 1.5921, 'learning_rate': 8.573106457669678e-06, 'epoch': 2.86}\n",
      "{'loss': 1.4608, 'learning_rate': 8.162504821834296e-06, 'epoch': 2.93}\n",
      "{'loss': 1.5431, 'learning_rate': 7.755085913624274e-06, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e1d6562ea44101b5cc232250edda84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8608595132827759, 'eval_accuracy': 0.3931324456902593, 'eval_f1_weighted': 0.376677507484159, 'eval_f1_macro': 0.3739792335467585, 'eval_runtime': 76.663, 'eval_samples_per_second': 37.228, 'eval_steps_per_second': 0.587, 'epoch': 3.0}\n",
      "{'loss': 1.3861, 'learning_rate': 7.351555423824361e-06, 'epoch': 3.05}\n",
      "{'loss': 1.3596, 'learning_rate': 6.952612308085057e-06, 'epoch': 3.11}\n",
      "{'loss': 1.3252, 'learning_rate': 6.558947576260705e-06, 'epoch': 3.18}\n",
      "{'loss': 1.3764, 'learning_rate': 6.171243095510463e-06, 'epoch': 3.24}\n",
      "{'loss': 1.278, 'learning_rate': 5.790170409235387e-06, 'epoch': 3.3}\n",
      "{'loss': 1.3932, 'learning_rate': 5.416389573897269e-06, 'epoch': 3.36}\n",
      "{'loss': 1.2705, 'learning_rate': 5.050548015734069e-06, 'epoch': 3.42}\n",
      "{'loss': 1.3623, 'learning_rate': 4.6932794093521215e-06, 'epoch': 3.49}\n",
      "{'loss': 1.3584, 'learning_rate': 4.345202580137597e-06, 'epoch': 3.55}\n",
      "{'loss': 1.3409, 'learning_rate': 4.0069204323882826e-06, 'epoch': 3.61}\n",
      "{'loss': 1.3834, 'learning_rate': 3.679018905022329e-06, 'epoch': 3.67}\n",
      "{'loss': 1.2946, 'learning_rate': 3.3620659566727433e-06, 'epoch': 3.74}\n",
      "{'loss': 1.3156, 'learning_rate': 3.056610581925551e-06, 'epoch': 3.8}\n",
      "{'loss': 1.3007, 'learning_rate': 2.7631818604055894e-06, 'epoch': 3.86}\n",
      "{'loss': 1.3376, 'learning_rate': 2.4822880403570783e-06, 'epoch': 3.92}\n",
      "{'loss': 1.3344, 'learning_rate': 2.214415658306199e-06, 'epoch': 3.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1140d49b176844d2aec6c181e4d4be83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.882143497467041, 'eval_accuracy': 0.3959355290819902, 'eval_f1_weighted': 0.38248937520401444, 'eval_f1_macro': 0.3729371664626534, 'eval_runtime': 73.4421, 'eval_samples_per_second': 38.861, 'eval_steps_per_second': 0.613, 'epoch': 4.0}\n",
      "{'loss': 1.2699, 'learning_rate': 1.960028696330596e-06, 'epoch': 4.05}\n",
      "{'loss': 1.2605, 'learning_rate': 1.719567778395449e-06, 'epoch': 4.11}\n",
      "{'loss': 1.2403, 'learning_rate': 1.493449407148182e-06, 'epoch': 4.17}\n",
      "{'loss': 1.1478, 'learning_rate': 1.282065242493713e-06, 'epoch': 4.23}\n",
      "{'loss': 1.1927, 'learning_rate': 1.0857814231998664e-06, 'epoch': 4.3}\n",
      "{'loss': 1.2776, 'learning_rate': 9.049379327079543e-07, 'epoch': 4.36}\n",
      "{'loss': 1.2024, 'learning_rate': 7.39848010247064e-07, 'epoch': 4.42}\n",
      "{'loss': 1.135, 'learning_rate': 5.907976082719958e-07, 'epoch': 4.48}\n",
      "{'loss': 1.2842, 'learning_rate': 4.5804489716467895e-07, 'epoch': 4.55}\n",
      "{'loss': 1.2247, 'learning_rate': 3.418198180569332e-07, 'epoch': 4.61}\n",
      "{'loss': 1.2678, 'learning_rate': 2.4232368454915834e-07, 'epoch': 4.67}\n",
      "{'loss': 1.2346, 'learning_rate': 1.597288340148051e-07, 'epoch': 4.73}\n",
      "{'loss': 1.2492, 'learning_rate': 9.417832909459856e-08, 'epoch': 4.79}\n",
      "{'loss': 1.2187, 'learning_rate': 4.578570989756559e-08, 'epoch': 4.86}\n",
      "{'loss': 1.1989, 'learning_rate': 1.4634797338074492e-08, 'epoch': 4.92}\n",
      "{'loss': 1.2129, 'learning_rate': 7.795479495298352e-10, 'epoch': 4.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2709fefab6e44fadbd11f0521e6f64da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8837237358093262, 'eval_accuracy': 0.39558514365802383, 'eval_f1_weighted': 0.38009256548285597, 'eval_f1_macro': 0.37563646567185055, 'eval_runtime': 83.947, 'eval_samples_per_second': 33.998, 'eval_steps_per_second': 0.536, 'epoch': 5.0}\n",
      "{'train_runtime': 6326.1584, 'train_samples_per_second': 20.299, 'train_steps_per_second': 0.635, 'train_loss': 1.7006105881402382, 'epoch': 5.0}\n",
      "Evaluating best checkpointâ€¦\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790dc3361f774aee88e635cf7cbe85f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss: 1.8821\n",
      "eval_accuracy: 0.3959\n",
      "eval_f1_weighted: 0.3825\n",
      "eval_f1_macro: 0.3729\n",
      "eval_runtime: 81.1000\n",
      "eval_samples_per_second: 35.1910\n",
      "eval_steps_per_second: 0.5550\n",
      "epoch: 5.0000\n",
      "Saving to: C:/SonicAid_clean/saved_models/roberta-goemotion-final\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Triton.*\")\n",
    "\n",
    "# Paths and basic setup\n",
    "CSV_PATH = r\"C:/SonicAid_clean/data/raw/goemotions_df.csv\"  # columns: text,label\n",
    "RUN_DIR  = r\"C:/SonicAid_clean/part1_emotion/roberta-goemotion-run\"\n",
    "SAVE_DIR = r\"C:/SonicAid_clean/saved_models/roberta-goemotion-final\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 1) Load and de-duplicate by text (keep majority label)\n",
    "df = pd.read_csv(CSV_PATH).dropna(subset=[\"text\",\"label\"]).reset_index(drop=True)\n",
    "df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
    "\n",
    "def majority(s):\n",
    "    vc = s.value_counts()\n",
    "    return vc.index[0]\n",
    "\n",
    "df_unique = df.groupby(\"text\", as_index=False)[\"label\"].apply(majority)\n",
    "print(f\"Original rows: {len(df)}, unique by text: {len(df_unique)}\")\n",
    "\n",
    "# Sample half per label for a quicker run (still stratified)\n",
    "df_half = (\n",
    "    df_unique.groupby(\"label\", group_keys=False)\n",
    "    .apply(lambda g: g.sample(frac=0.5, random_state=SEED) if len(g) > 1 else g)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(f\"Using half of unique set: {len(df_half)} rows\")\n",
    "\n",
    "# 2) Encode labels and split\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(df_half[\"label\"])\n",
    "num_labels = len(le.classes_)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_half[\"text\"], y_all, test_size=0.1, random_state=SEED, stratify=y_all\n",
    ")\n",
    "\n",
    "# 3) Tokenizer / model\n",
    "ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(ckpt, num_labels=num_labels)\n",
    "\n",
    "# Store label maps in config for consistent reloads\n",
    "id2label = {int(i): lab for i, lab in enumerate(le.classes_.tolist())}\n",
    "label2id = {lab: int(i) for i, lab in id2label.items()}\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "model.config.problem_type = \"single_label_classification\"\n",
    "\n",
    "# 4) Build HF datasets\n",
    "train_ds = Dataset.from_dict({\"text\": train_texts.tolist(), \"label\": train_labels})\n",
    "val_ds   = Dataset.from_dict({\"text\": val_texts.tolist(),   \"label\": val_labels})\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=False,          # dynamic padding via collator\n",
    "        truncation=True,\n",
    "        max_length=48           # p99 â‰ˆ 37; 48 is safe\n",
    "    )\n",
    "\n",
    "train_tok = train_ds.map(tok_fn, batched=True)\n",
    "val_tok   = val_ds.map(tok_fn,   batched=True)\n",
    "\n",
    "# Keep only what the model needs and set torch format\n",
    "keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_tok = train_tok.remove_columns([c for c in train_tok.column_names if c not in keep]).with_format(\"torch\")\n",
    "val_tok   = val_tok.remove_columns([c for c in val_tok.column_names   if c not in keep]).with_format(\"torch\")\n",
    "\n",
    "# Dynamic padding (pad to multiple of 8 to help tensor cores)\n",
    "collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# 5) Classâ€‘weighted loss to handle imbalance\n",
    "cls_weights = compute_class_weight(class_weight=\"balanced\",\n",
    "                                   classes=np.arange(num_labels),\n",
    "                                   y=train_labels)\n",
    "cls_weights = torch.tensor(cls_weights, dtype=torch.float)\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = torch.nn.CrossEntropyLoss(\n",
    "            weight=self.class_weights.to(model.device) if self.class_weights is not None else None\n",
    "        )\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 6) Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "# 7) Training args (lean but solid)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=RUN_DIR,\n",
    "    per_device_train_batch_size=16,   # shorter seq â†’ higher batch ok\n",
    "    per_device_eval_batch_size=64,\n",
    "    gradient_accumulation_steps=2,    # effective train batch â‰ˆ 32\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_weighted\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# 8) Train, evaluate, save\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=cls_weights,\n",
    ")\n",
    "\n",
    "print(\"Starting trainingâ€¦\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"Evaluating best checkpointâ€¦\")\n",
    "metrics = trainer.evaluate()\n",
    "for k, v in metrics.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(f\"Saving to: {SAVE_DIR}\")\n",
    "trainer.save_model(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"label_classes.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(list(le.classes_), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a715c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p90,p95,p99: [31. 33. 37.]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "df = pd.read_csv(r\"C:/SonicAid_clean/data/raw/goemotions_df.csv\").dropna(subset=[\"text\"])\n",
    "lens = [len(tokenizer(t, truncation=False)[\"input_ids\"]) for t in df[\"text\"].tolist()[:5000]]  # sample\n",
    "print(\"p90,p95,p99:\", np.percentile(lens, [90,95,99]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'I feel so hopeless and tired of everything.', 'distress': True, 'distress_conf': 0.9791, 'top_emotions': [('sadness', 0.5292), ('disappointment', 0.321), ('annoyance', 0.022), ('nervousness', 0.0165), ('disgust', 0.0155)], 'filtered_emotions': [('sadness', 0.5292), ('disappointment', 0.321), ('nervousness', 0.0165), ('disgust', 0.0155)], 'chosen_emotion_fine': 'sadness', 'chosen_emotion_coarse': 'sadness', 'chosen_prob': 0.5292}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "GOEMO_PATH = r\"C:/SonicAid_clean/saved_models/roberta-goemotion-final\"\n",
    "DISTRESS_PATH = r\"C:/SonicAid_clean/saved_models/distress_classifier\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return str(s).strip().lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "# Load GoEmotions model/tokenizer (try fast tokenizer first)\n",
    "goemotions_model = AutoModelForSequenceClassification.from_pretrained(GOEMO_PATH).to(device).eval()\n",
    "try:\n",
    "    goemotions_tokenizer = AutoTokenizer.from_pretrained(GOEMO_PATH, use_fast=True, local_files_only=True)\n",
    "except Exception:\n",
    "    goemotions_tokenizer = AutoTokenizer.from_pretrained(GOEMO_PATH, use_fast=False, local_files_only=True)\n",
    "\n",
    "# Load distress model/tokenizer\n",
    "distress_model = AutoModelForSequenceClassification.from_pretrained(DISTRESS_PATH).to(device).eval()\n",
    "try:\n",
    "    distress_tokenizer = AutoTokenizer.from_pretrained(DISTRESS_PATH, use_fast=True, local_files_only=True)\n",
    "except Exception:\n",
    "    distress_tokenizer = AutoTokenizer.from_pretrained(DISTRESS_PATH, use_fast=False, local_files_only=True)\n",
    "\n",
    "# Resolve id2label (prefer saved label_classes.json)\n",
    "id2label = None\n",
    "label_json = os.path.join(GOEMO_PATH, \"label_classes.json\")\n",
    "if os.path.isfile(label_json):\n",
    "    with open(label_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        classes = json.load(f)\n",
    "    id2label = {i: classes[i] for i in range(len(classes))}\n",
    "else:\n",
    "    cfg = goemotions_model.config\n",
    "    if isinstance(cfg.id2label, dict) and len(cfg.id2label) == cfg.num_labels:\n",
    "        names = [cfg.id2label.get(str(i), cfg.id2label.get(i, f\"label_{i}\")) for i in range(cfg.num_labels)]\n",
    "        id2label = {i: names[i] for i in range(len(names))}\n",
    "\n",
    "assert id2label is not None, \"Could not resolve id2label for GoEmotions model.\"\n",
    "label_names = [id2label[i] for i in range(len(id2label))]\n",
    "label_names_norm = [_norm(x) for x in label_names]\n",
    "\n",
    "# Fine â†’ coarse buckets (for music prompts)\n",
    "FINE_TO_COARSE = {\n",
    "    \"joy\": {\"amusement\",\"excitement\",\"joy\",\"optimism\",\"pride\",\"approval\"},\n",
    "    \"sadness\": {\"sadness\",\"disappointment\",\"grief\",\"remorse\"},\n",
    "    \"anger\": {\"anger\",\"annoyance\",\"disgust\"},\n",
    "    \"fear\": {\"fear\",\"nervousness\"},\n",
    "    \"love\": {\"love\",\"caring\",\"admiration\"},\n",
    "    \"gratitude\": {\"gratitude\"},\n",
    "    \"neutral\": {\"neutral\",\"relief\",\"realization\",\"curiosity\",\"confusion\",\"surprise\",\"desire\",\"embarrassment\",\"disapproval\"},\n",
    "}\n",
    "FINE_TO_COARSE = {k: {_norm(v) for v in vals} for k, vals in FINE_TO_COARSE.items()}\n",
    "\n",
    "def fine_to_coarse(label_str: str) -> str:\n",
    "    n = _norm(label_str)\n",
    "    for coarse, fines in FINE_TO_COARSE.items():\n",
    "        if n in fines:\n",
    "            return coarse\n",
    "    return \"neutral\"\n",
    "\n",
    "# Distress vs nonâ€‘distress groups (normalized)\n",
    "distress_emotions = {_norm(x) for x in {\"anger\",\"fear\",\"grief\",\"remorse\",\"sadness\",\"disappointment\",\"nervousness\",\"disgust\",\"disapproval\"}}\n",
    "non_distress_emotions = {_norm(x) for x in {\"joy\",\"love\",\"gratitude\",\"optimism\",\"amusement\",\"excitement\",\"surprise\",\"relief\",\"admiration\",\"approval\",\"caring\"}}\n",
    "\n",
    "@torch.no_grad()\n",
    "def hybrid_predict(text: str, topk: int = 5, max_length: int = 96):\n",
    "    # 1) Distress classifier\n",
    "    di = distress_tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "    dlogits = distress_model(**di).logits\n",
    "    dprobs = torch.softmax(dlogits, dim=1)\n",
    "    is_distress = int(torch.argmax(dprobs, dim=1).item())\n",
    "    distress_conf = float(dprobs[0, is_distress].item())\n",
    "\n",
    "    # 2) GoEmotions classifier\n",
    "    gi = goemotions_tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "    logits = goemotions_model(**gi).logits\n",
    "    probs = torch.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "\n",
    "    # 3) Topâ€‘k fine labels\n",
    "    k = min(topk, len(probs))\n",
    "    top_idx = np.argsort(probs)[::-1][:k]\n",
    "    top_emotions = [(label_names[i], float(probs[i])) for i in top_idx]\n",
    "    top_norm = [(_norm(label_names[i]), float(probs[i])) for i in top_idx]\n",
    "\n",
    "    # 4) Filter by distress flag (ensure nonâ€‘empty)\n",
    "    if is_distress:\n",
    "        filtered = [(lab, p) for lab, p in top_norm if lab in distress_emotions]\n",
    "    else:\n",
    "        filtered = [(lab, p) for lab, p in top_norm if lab in non_distress_emotions]\n",
    "    if not filtered:\n",
    "        filtered = [top_norm[0]]\n",
    "\n",
    "    # 5) Pick final label and coarse bucket\n",
    "    chosen_fine_norm, chosen_prob = filtered[0]\n",
    "    try:\n",
    "        idx = label_names_norm.index(chosen_fine_norm)\n",
    "        chosen_fine = label_names[idx]\n",
    "    except ValueError:\n",
    "        chosen_fine = chosen_fine_norm\n",
    "    chosen_coarse = fine_to_coarse(chosen_fine)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"distress\": bool(is_distress),\n",
    "        \"distress_conf\": round(distress_conf, 4),\n",
    "        \"top_emotions\": [(label_names[i], round(float(probs[i]), 4)) for i in top_idx],\n",
    "        \"filtered_emotions\": [(lab, round(p,4)) for lab, p in filtered],\n",
    "        \"chosen_emotion_fine\": chosen_fine,\n",
    "        \"chosen_emotion_coarse\": chosen_coarse,\n",
    "        \"chosen_prob\": round(float(chosen_prob), 4),\n",
    "    }\n",
    "\n",
    "# Quick demo\n",
    "if __name__ == \"__main__\":\n",
    "    sample = \"I feel so hopeless and tired of everything.\"\n",
    "    out = hybrid_predict(sample)\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a36e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Text: Everything feels overwhelming and out of control lately.\n",
      "ðŸ˜” Detected Distress:\n",
      "\n",
      "Top Emotions: [('disappointment', 0.3716), ('sadness', 0.1356), ('annoyance', 0.1336), ('nervousness', 0.1001), ('neutral', 0.0719)]\n",
      "ðŸŽ¯ Filtered Emotions: [('disappointment', 0.3716), ('sadness', 0.1356), ('nervousness', 0.1001)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Distress (binary) model\n",
    "distress_model = AutoModelForSequenceClassification.from_pretrained(\"saved_models/distress_classifier\").to(device)\n",
    "distress_tokenizer = AutoTokenizer.from_pretrained(\"saved_models/distress_classifier\")\n",
    "\n",
    "# GoEmotions (multi-class, DistilBERT)\n",
    "goemotions_model = AutoModelForSequenceClassification.from_pretrained(\"saved_models/goemotions_distilbert_v3\").to(device)\n",
    "goemotions_tokenizer = AutoTokenizer.from_pretrained(\"saved_models/goemotions_distilbert_v3\")\n",
    "\n",
    "# Label map for GoEmotions\n",
    "id2emotion = {\n",
    "    0: \"amusement\", 1: \"anger\", 2: \"annoyance\", 3: \"approval\", 4: \"caring\", 5: \"confusion\", 6: \"curiosity\",\n",
    "    7: \"desire\", 8: \"disappointment\", 9: \"disapproval\", 10: \"disgust\", 11: \"embarrassment\", 12: \"excitement\",\n",
    "    13: \"fear\", 14: \"gratitude\", 15: \"grief\", 16: \"joy\", 17: \"love\", 18: \"nervousness\", 19: \"neutral\",\n",
    "    20: \"optimism\", 21: \"pride\", 22: \"realization\", 23: \"relief\", 24: \"remorse\", 25: \"sadness\", 26: \"surprise\"\n",
    "}\n",
    "\n",
    "# Simple buckets for filtering\n",
    "distress_emotions = {\"anger\", \"fear\", \"grief\", \"remorse\", \"sadness\", \"disappointment\", \"nervousness\"}\n",
    "non_distress_emotions = {\"joy\", \"love\", \"gratitude\", \"optimism\", \"amusement\", \"excitement\", \"surprise\"}\n",
    "\n",
    "def hybrid_predict(text):\n",
    "    # Distress prediction\n",
    "    dist_input = distress_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        dist_output = distress_model(**dist_input)\n",
    "        is_distress = torch.argmax(dist_output.logits, dim=1).item()\n",
    "\n",
    "    # GoEmotions prediction\n",
    "    goemo_input = goemotions_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        goemo_output = goemotions_model(**goemo_input)\n",
    "        probs = torch.softmax(goemo_output.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    # Topâ€‘5 emotions\n",
    "    top_indices = np.argsort(probs)[::-1][:5]\n",
    "    top_emotions = [(id2emotion[i], round(probs[i], 4)) for i in top_indices]\n",
    "\n",
    "    # Filter by distress flag\n",
    "    if is_distress:\n",
    "        filtered = [(emo, prob) for emo, prob in top_emotions if emo in distress_emotions]\n",
    "    else:\n",
    "        filtered = [(emo, prob) for emo, prob in top_emotions if emo in non_distress_emotions]\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"distress\": bool(is_distress),\n",
    "        \"top_emotions\": top_emotions,\n",
    "        \"filtered_emotions\": filtered\n",
    "    }\n",
    "\n",
    "# Example\n",
    "sample = \"Everything feels overwhelming and out of control lately.\"\n",
    "result = hybrid_predict(sample)\n",
    "\n",
    "print(\"\\nText:\", result[\"text\"])\n",
    "print(\"Detected Distress:\" if result[\"distress\"] else \"ðŸ™‚ No Distress Detected\")\n",
    "print(\"\\nTop Emotions:\", result[\"top_emotions\"])\n",
    "print(\"Filtered Emotions:\", result[\"filtered_emotions\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sonicaid)",
   "language": "python",
   "name": "sonicaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

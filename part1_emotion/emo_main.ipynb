{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, gc, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Core data tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hugging Face\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    default_data_collator,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from evaluate import load as load_metric  # optional\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Audio\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "import music21\n",
    "\n",
    "# UI\n",
    "import streamlit as st\n",
    "\n",
    "# pydub â†” ffmpeg (use system/conda ffmpeg if available)\n",
    "AudioSegment.converter = which(\"ffmpeg\") or AudioSegment.converter\n",
    "\n",
    "# Quiet a few noisy-but-harmless warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Triton.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*weight_norm is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated.*\")\n",
    "\n",
    "# Version sanity checks (comment out if you prefer flexibility)\n",
    "assert torch.__version__.startswith(\"2.1.0\"), f\"Torch pin expected 2.1.0, got {torch.__version__}\"\n",
    "assert transformers.__version__.startswith(\"4.37.\"), f\"Transformers pin expected 4.37.x, got {transformers.__version__}\"\n",
    "import numpy  # keep separate to check the exact package version below\n",
    "assert numpy.__version__ == \"1.26.4\", f\"Numpy pin expected 1.26.4, got {numpy.__version__}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86dd5651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoEmotions 1: (70000, 37)\n",
      "                                                 text       id       author  \\\n",
      "0                                    That game hurt.  eew5j0j        Brdd9   \n",
      "1   >sexuality shouldnâ€™t be a grouping category I...  eemcysk  TheGreen888   \n",
      "\n",
      "          subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0               nrl  t3_ajis4z  t1_eew18eq  1.548381e+09         1   \n",
      "1  unpopularopinion  t3_ai4q37   t3_ai4q37  1.548084e+09        37   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         0      0   \n",
      "1                  True           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        1         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "GoEmotions 2: (70000, 37)\n",
      "                              text       id             author  \\\n",
      "0                     We can hope  ee3o3ko      darkenseyreth   \n",
      "1  Shhh don't give them the idea!  eebl3z7  BoinkBoinkEtAliae   \n",
      "\n",
      "         subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0   EdmontonOilers  t3_ag4r9j  t1_ee3mhad  1.547529e+09        62   \n",
      "1  MurderedByWords  t3_ah3o76  t1_eeb68lo  1.547777e+09        76   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         1      0   \n",
      "1                 False           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        0         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "GoEmotions 3: (71225, 37)\n",
      "                                                 text       id        author  \\\n",
      "0  Worst ending ever! I won't spoil it but this o...  eee021b    tribaltrak   \n",
      "1                     Happy cake day u/sneakpeekbot!  ed00f1z  InfernicFuse   \n",
      "\n",
      "     subreddit    link_id   parent_id   created_utc  rater_id  \\\n",
      "0       movies  t3_agxbsx   t3_agxbsx  1.547847e+09        42   \n",
      "1  danganronpa  t3_abajdo  t1_eczoocj  1.546328e+09        24   \n",
      "\n",
      "   example_very_unclear  admiration  ...  love  nervousness  optimism  pride  \\\n",
      "0                 False           0  ...     0            0         0      0   \n",
      "1                 False           0  ...     0            0         0      0   \n",
      "\n",
      "   realization  relief  remorse  sadness  surprise  neutral  \n",
      "0            0       0        0        0         0        0  \n",
      "1            0       0        0        0         0        0  \n",
      "\n",
      "[2 rows x 37 columns] \n",
      "\n",
      "Column consistency check: True\n",
      "Columns: ['text', 'id', 'author', 'subreddit', 'link_id', 'parent_id', 'created_utc', 'rater_id', 'example_very_unclear', 'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
      " Combined GoEmotions shape: (211225, 37)\n",
      "                                                     text  love  nervousness  \\\n",
      "126101         oh [NAME], the fumbles.... all the fumbles     0            0   \n",
      "67757     If the fan doesn't work, then I'm fine with it.     0            0   \n",
      "168566  This isn't even co-optation this just out righ...     0            0   \n",
      "\n",
      "        optimism  pride  realization  relief  remorse  sadness  surprise  \\\n",
      "126101         0      0            0       0        0        0         0   \n",
      "67757          0      0            0       0        0        0         0   \n",
      "168566         0      0            0       0        0        0         0   \n",
      "\n",
      "        neutral  \n",
      "126101        0  \n",
      "67757         0  \n",
      "168566        0  \n"
     ]
    }
   ],
   "source": [
    "data_path = \"C:/SonicAid_clean/data/raw\"\n",
    "# Load GoEmotions parts\n",
    "geo1 = pd.read_csv(os.path.join(data_path, \"goemotions_1.csv\"))\n",
    "geo2 = pd.read_csv(os.path.join(data_path, \"goemotions_2.csv\"))\n",
    "geo3 = pd.read_csv(os.path.join(data_path, \"goemotions_3.csv\"))\n",
    "\n",
    "# Preview each\n",
    "print(f\"GoEmotions 1: {geo1.shape}\\n\", geo1.head(2), \"\\n\")\n",
    "print(f\"GoEmotions 2: {geo2.shape}\\n\", geo2.head(2), \"\\n\")\n",
    "print(f\"GoEmotions 3: {geo3.shape}\\n\", geo3.head(2), \"\\n\")\n",
    "\n",
    "# Confirm columns are the same\n",
    "print(\"Column consistency check:\", geo1.columns.equals(geo2.columns) and geo1.columns.equals(geo3.columns))\n",
    "print(\"Columns:\", geo1.columns.tolist())\n",
    "\n",
    "# Combine all three\n",
    "goemotions_df = pd.concat([geo1, geo2, geo3], ignore_index=True)\n",
    "print(\" Combined GoEmotions shape:\", goemotions_df.shape)\n",
    "print(goemotions_df[['text'] + list(goemotions_df.columns[-10:])].sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7da84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned GoEmotions sample:\n",
      "                                                    text      label\n",
      "158821        [NAME] still looks fresh for 40 years old.  gratitude\n",
      "103196        They just keep coming, it's like a cartoon    neutral\n",
      "197092     TYM is the best place for that kind of thing.    neutral\n",
      "1528    Water? Find a way to keep your temperature down.     caring\n",
      "101954   It did. MM used way more textures than Ocarina.   approval\n",
      "Shape: (207814, 2)\n",
      "âœ… Cleaned Dreaddit sample:\n",
      "                                                   text        label\n",
      "3318  I don't remember always being like this, but o...  no_distress\n",
      "3125  Are there federal or state (IN) laws that gove...  no_distress\n",
      "2696  I just feel kinda gross because I was giving h...     distress\n",
      "47    Dental Lifeline Network's Donated Dental Servi...  no_distress\n",
      "3053  Iâ€™m not afraid of this guy, at all, and I have...     distress\n",
      "Shape: (3553, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.makedirs(\"C:/SonicAid_clean/data/raw\", exist_ok=True)\n",
    "\n",
    "# ensure boolean type for this flag\n",
    "if goemotions_df[\"example_very_unclear\"].dtype == object:\n",
    "    goemotions_df[\"example_very_unclear\"] = goemotions_df[\"example_very_unclear\"].astype(str).str.lower().map({\"true\": True, \"false\": False})\n",
    "\n",
    "# drop unclear rows\n",
    "goemotions_df = goemotions_df[goemotions_df[\"example_very_unclear\"] == False].reset_index(drop=True)\n",
    "\n",
    "# detect emotion columns: columns that are all in {0,1,True,False} (ignoring NA)\n",
    "def is_binary_series(s):\n",
    "    s2 = s.dropna()\n",
    "    return s2.isin([0,1,True,False]).all()\n",
    "\n",
    "candidate_cols = [c for c in goemotions_df.columns if is_binary_series(goemotions_df[c])]\n",
    "# common non-emotion binaries to exclude\n",
    "exclude = {\"example_very_unclear\"}\n",
    "emotion_columns = [c for c in candidate_cols if c not in exclude]\n",
    "\n",
    "# coerce to numeric 0/1\n",
    "goemotions_df[emotion_columns] = goemotions_df[emotion_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# keep rows with â‰¥1 emotion\n",
    "mask_has_emotion = goemotions_df[emotion_columns].sum(axis=1) > 0\n",
    "goemotions_df = goemotions_df[mask_has_emotion].reset_index(drop=True)\n",
    "\n",
    "# single-label: take the first active emotion (or argmax)\n",
    "goemotions_df[\"label\"] = goemotions_df[emotion_columns].idxmax(axis=1)\n",
    "\n",
    "# final tidy\n",
    "goemotions_clean = goemotions_df[[\"text\", \"label\"]].dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Cleaned GoEmotions sample:\")\n",
    "print(goemotions_clean.sample(min(5, len(goemotions_clean))))\n",
    "print(\"Shape:\", goemotions_clean.shape)\n",
    "\n",
    "goemotions_clean.to_csv(\"C:/SonicAid_clean/data/raw/goemotions_df.csv\", index=False)\n",
    "\n",
    "#  Clean Dreaddit Dataset\n",
    "\n",
    "# Load train/test files\n",
    "df_train = pd.read_csv(\"C:/SonicAid_clean/data/raw/dreaddit-train.csv\")\n",
    "df_test = pd.read_csv(\"C:/SonicAid_clean/data/raw/dreaddit-test.csv\")\n",
    "\n",
    "# Combine and filter\n",
    "dreaddit_full = pd.concat([df_train, df_test], ignore_index=True)[[\"text\", \"label\"]]\n",
    "dreaddit_full.dropna(subset=[\"text\", \"label\"], inplace=True)\n",
    "\n",
    "# Map to readable labels\n",
    "dreaddit_full[\"label\"] = dreaddit_full[\"label\"].map({1: \"distress\", 0: \"no_distress\"})\n",
    "\n",
    "# Final check\n",
    "print(\"Cleaned Dreaddit sample:\")\n",
    "print(dreaddit_full.sample(5))\n",
    "print(\"Shape:\", dreaddit_full.shape)\n",
    "\n",
    "# Save cleaned file\n",
    "dreaddit_full.to_csv(\"C:/SonicAid_clean/data/raw/dreaddit_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "goemotions_df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "goemotions_df['label'] = goemotions_df['label'].astype('category')\n",
    "goemotions_df['label_id'] = goemotions_df['label'].cat.codes\n",
    "\n",
    "label2id = dict(enumerate(goemotions_df['label'].cat.categories))\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "goemotions_dataset = Dataset.from_pandas(\n",
    "goemotions_df[['text', 'label_id']].rename(columns={'label_id': 'label'})\n",
    ")\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        checkpoint,\n",
    "        num_labels=len(label2id),\n",
    "        id2label=label2id,\n",
    "        label2id=id2label\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"Using device:\", model.device)\n",
    "\n",
    "def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dataset = goemotions_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        return {\n",
    "            \"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "        }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./results_goemotions_v2\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs_goemotions_v2\",\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        logging_steps=50,\n",
    "        save_steps=500\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "    # 8. Save the trained model\n",
    "save_path = \"./saved_models/goemotions_distilbert_v2\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"Model and tokenizer saved to: {save_path}\")\n",
    "\n",
    "# 8. Save the trained model and tokenizer\n",
    "save_path = \"./saved_models/goemotions_distilbert_v3\"\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94dd4d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"C:/SonicAid_clean/saved_models/goemotions_distilbert_v3\"\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71395da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dreaddit_df = pd.read_csv(\"C:/SonicAid_clean/data/raw/dreaddit_df.csv\")[['text', 'label']].dropna()\n",
    "dreaddit_df[\"label\"] = dreaddit_df[\"label\"].map({\"no_distress\": 0, \"distress\": 1})\n",
    "\n",
    "# 2. Convert to HuggingFace Dataset\n",
    "dreaddit_dataset = Dataset.from_pandas(dreaddit_df)\n",
    "\n",
    "# 3. Tokenization\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer_dreaddit = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer_dreaddit(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized_dreaddit = dreaddit_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dreaddit = tokenized_dreaddit.train_test_split(test_size=0.1)\n",
    "\n",
    "# 4. Load model (binary classification) and move to GPU\n",
    "model_dreaddit = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint, num_labels=2\n",
    ").to(\"cuda\")\n",
    "\n",
    "# 5. Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# 6. Training setup\n",
    "training_args_dreaddit = TrainingArguments(\n",
    "    output_dir=\"./results_dreaddit\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_dreaddit\"\n",
    ")\n",
    "\n",
    "trainer_dreaddit = Trainer(\n",
    "    model=model_dreaddit,\n",
    "    args=training_args_dreaddit,\n",
    "    train_dataset=tokenized_dreaddit[\"train\"],\n",
    "    eval_dataset=tokenized_dreaddit[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer_dreaddit\n",
    ")\n",
    "\n",
    "# 7. Train the model\n",
    "trainer_dreaddit.train()\n",
    "\n",
    "# 8. Save model and tokenizer\n",
    "save_path = \"saved_models/distress_classifier\"\n",
    "trainer_dreaddit.save_model(save_path)\n",
    "tokenizer_dreaddit.save_pretrained(save_path)\n",
    "print(f\"Distress model saved to: {save_path}\")\n",
    "\n",
    "# 9. Evaluate the model\n",
    "metrics_dreaddit = trainer_dreaddit.evaluate()\n",
    "print(\"Distress Model Evaluation Results:\")\n",
    "for k, v in metrics_dreaddit.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# 10. Optional inference example\n",
    "sample_text = [\"I'm feeling overwhelmed and anxious.\"]\n",
    "inputs = tokenizer_dreaddit(sample_text, return_tensors=\"pt\").to(model_dreaddit.device)\n",
    "outputs = model_dreaddit(**inputs)\n",
    "pred = torch.argmax(outputs.logits, dim=1)\n",
    "print(f\"redicted label: {'distress' if pred.item() == 1 else 'no_distress'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec2c805eeb5d496c9e05596a8e78f858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'DataCollatorWithPadding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m tokenized \u001b[38;5;241m=\u001b[39m tokenized\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Dynamic padding happens in the collator (saves lots of memory)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m collator \u001b[38;5;241m=\u001b[39m \u001b[43mDataCollatorWithPadding\u001b[49m(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_dreaddit)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 3) Model â€” disable cache to save memory during training\u001b[39;00m\n\u001b[0;32m     23\u001b[0m model_dreaddit \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DataCollatorWithPadding' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: helps fragmentation on small GPUs (set BEFORE torch allocates memory)\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb:128\")\n",
    "\n",
    "# 1) Load data\n",
    "dreaddit_df = pd.read_csv(\"C:/SonicAid_clean/data/raw/dreaddit_df.csv\")[['text', 'label']].dropna()\n",
    "dreaddit_df[\"label\"] = dreaddit_df[\"label\"].map({\"no_distress\": 0, \"distress\": 1}).astype(int)\n",
    "dreaddit_dataset = Dataset.from_pandas(dreaddit_df)\n",
    "\n",
    "# 2) Tokenizer  DO NOT pad here; set a lower max_length\n",
    "checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer_dreaddit = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer_dreaddit(batch[\"text\"], truncation=True, max_length=128) \n",
    "\n",
    "tokenized = dreaddit_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dynamic padding happens in the collator (saves lots of memory)\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer_dreaddit)\n",
    "\n",
    "# 3) Model â€” disable cache to save memory during training\n",
    "model_dreaddit = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "model_dreaddit.config.use_cache = False  # important for training memory\n",
    "model_dreaddit.to(\"cuda\")\n",
    "\n",
    "# 4) Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds),\n",
    "            \"f1\": f1_score(labels, preds)}\n",
    "\n",
    "# 5) Training args smaller batches, fp16, grad checkpointing, eval accumulation\n",
    "training_args_dreaddit = TrainingArguments(\n",
    "    output_dir=\"./results_dreaddit\",\n",
    "    per_device_train_batch_size=4,       \n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True, \n",
    "    eval_accumulation_steps=2, \n",
    "    logging_dir=\"./logs_dreaddit\",\n",
    "    dataloader_num_workers=0,\n",
    ")\n",
    "\n",
    "trainer_dreaddit = Trainer(\n",
    "    model=model_dreaddit,\n",
    "    args=training_args_dreaddit,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer_dreaddit,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "trainer_dreaddit.train()\n",
    "\n",
    "# Save & evaluate\n",
    "save_path = \"saved_models/distress_classifier\"\n",
    "trainer_dreaddit.save_model(save_path)\n",
    "tokenizer_dreaddit.save_pretrained(save_path)\n",
    "print(f\"Distress model saved to: {save_path}\")\n",
    "\n",
    "metrics = trainer_dreaddit.evaluate()\n",
    "print(\"Distress Model Evaluation Results:\")\n",
    "for k, v in metrics.items():\n",
    "    try:\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    except Exception:\n",
    "        print(k, v)\n",
    "\n",
    "# Inference example\n",
    "sample_text = [\"I'm feeling overwhelmed and anxious.\"]\n",
    "inputs = tokenizer_dreaddit(sample_text, return_tensors=\"pt\").to(model_dreaddit.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model_dreaddit(**inputs)\n",
    "pred = torch.argmax(outputs.logits, dim=1)\n",
    "print(f\"Predicted label: {'distress' if pred.item() == 1 else 'no_distress'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load saved model and tokenizer\n",
    "save_path = \"C:/SonicAid_clean/saved_models/goemotions_distilbert_v3\"\n",
    "\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_path).to(\"cuda\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# 2. Load and preprocess dataset\n",
    "goemotions_df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "goemotions_df[\"label\"] = goemotions_df[\"label\"].astype(\"category\")\n",
    "goemotions_df[\"label_id\"] = goemotions_df[\"label\"].cat.codes\n",
    "\n",
    "goemotions_dataset = Dataset.from_pandas(\n",
    "    goemotions_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"})\n",
    ")\n",
    "\n",
    "# 3. Tokenization\n",
    "def tokenize(batch):\n",
    "    return loaded_tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized = goemotions_dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "tokenized = tokenized.train_test_split(test_size=0.1)\n",
    "\n",
    "# 4. Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# 5. Evaluation setup\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    do_eval=True,\n",
    "    logging_dir=\"./logs_eval\"\n",
    ")\n",
    "\n",
    "trainer_eval = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=loaded_tokenizer\n",
    ")\n",
    "\n",
    "# 6. Run evaluation\n",
    "results = trainer_eval.evaluate()\n",
    "\n",
    "# 7. Display results\n",
    "print(\"ðŸ“Š GoEmotions Model Evaluation (Reloaded):\")\n",
    "for k, v in results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "# 2. Label encoding\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "num_labels = len(le.classes_)\n",
    "\n",
    "# 3. Build HF Dataset\n",
    "dataset = Dataset.from_dict({\"text\": texts, \"label\": encoded_labels})\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# 4. Tokenizer and Model\n",
    "checkpoint = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
    "\n",
    "# 5. Tokenization\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# 6. Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = torch.argmax(torch.tensor(logits), dim=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# 7. Training args\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-goemotion-run\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    do_eval=True,\n",
    "    save_steps=500  \n",
    ")\n",
    "\n",
    "\n",
    "# 8. Trainer\n",
    "model.to(\"cuda\")\n",
    "trainer_rob = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 9. Train!\n",
    "trainer_rob.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cc769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "trainer_rob.save_model(\"C:/SonicAid_clean/saved_models/roberta-goemotion-final\")\n",
    "\n",
    "# Save tokenizer (must match the model)\n",
    "tokenizer.save_pretrained(\"C:/SonicAid_clean/saved_models/roberta-goemotion-final\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f1e2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 250370 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/SonicAid_clean/saved_models/roberta-goemotion-final\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m---> 10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/raw/goemotions_df.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:814\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    812\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m         )\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2030\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2031\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2032\u001b[0m     init_configuration,\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2034\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2035\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2036\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2037\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2038\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2040\u001b[0m )\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m     )\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta_fast.py:185\u001b[0m, in \u001b[0;36mRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, trim_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    165\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    179\u001b[0m ):\n\u001b[0;32m    180\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    181\u001b[0m         AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m    184\u001b[0m     )\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    186\u001b[0m         vocab_file,\n\u001b[0;32m    187\u001b[0m         merges_file,\n\u001b[0;32m    188\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    189\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    190\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    191\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    192\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    193\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    194\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    195\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    196\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    197\u001b[0m         add_prefix_space\u001b[38;5;241m=\u001b[39madd_prefix_space,\n\u001b[0;32m    198\u001b[0m         trim_offsets\u001b[38;5;241m=\u001b[39mtrim_offsets,\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    202\u001b[0m     pre_tok_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer\u001b[38;5;241m.\u001b[39m__getstate__())\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_tok_state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_prefix_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_prefix_space) \u001b[38;5;241m!=\u001b[39m add_prefix_space:\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 250370 column 3"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load\n",
    "model_path = \"C:/SonicAid_clean/saved_models/roberta-goemotion-final\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"data/raw/goemotions_df.csv\")\n",
    "df[\"label\"] = df[\"label\"].astype(\"category\")\n",
    "df[\"label_id\"] = df[\"label\"].cat.codes\n",
    "\n",
    "unique_labels = df[\"label\"].cat.categories.tolist()\n",
    "\n",
    "test_df = df.sample(frac=0.2, random_state=42)\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\", \"label_id\"]].rename(columns={\"label_id\": \"label\"}))\n",
    "\n",
    "# Tokenize with max_length=64\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=64)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "# TrainingArguments for eval\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./eval_fp16_shortseq\",\n",
    "    per_device_eval_batch_size=128,\n",
    "    fp16=True,  # Mixed precision\n",
    "    do_eval=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(\"cuda\"),\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Show\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"{k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee49bf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "data did not match any variant of untagged enum ModelWrapper at line 250370 column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load models\u001b[39;00m\n\u001b[0;32m     12\u001b[0m goemotions_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(goemotions_path)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 13\u001b[0m goemotions_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoemotions_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m distress_model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(distress_path)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     16\u001b[0m distress_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(distress_path)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:814\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    812\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m         )\n\u001b[1;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2029\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2027\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2030\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2031\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2032\u001b[0m     init_configuration,\n\u001b[0;32m   2033\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2034\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2035\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2036\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2037\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2038\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2040\u001b[0m )\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2261\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2261\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m     )\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta_fast.py:185\u001b[0m, in \u001b[0;36mRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, trim_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    165\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    179\u001b[0m ):\n\u001b[0;32m    180\u001b[0m     mask_token \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    181\u001b[0m         AddedToken(mask_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[0;32m    184\u001b[0m     )\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    186\u001b[0m         vocab_file,\n\u001b[0;32m    187\u001b[0m         merges_file,\n\u001b[0;32m    188\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    189\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    190\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    191\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    192\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    193\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    194\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    195\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    196\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    197\u001b[0m         add_prefix_space\u001b[38;5;241m=\u001b[39madd_prefix_space,\n\u001b[0;32m    198\u001b[0m         trim_offsets\u001b[38;5;241m=\u001b[39mtrim_offsets,\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    202\u001b[0m     pre_tok_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer\u001b[38;5;241m.\u001b[39m__getstate__())\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_tok_state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_prefix_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_prefix_space) \u001b[38;5;241m!=\u001b[39m add_prefix_space:\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\sonicaid\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:111\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: data did not match any variant of untagged enum ModelWrapper at line 250370 column 3"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Load both models and tokenizers\n",
    "goemotions_path = \"C:/SonicAid_clean/saved_models/roberta-goemotion-final\"\n",
    "distress_path = \"C:/SonicAid_clean/saved_models/distress_classifier\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load models\n",
    "goemotions_model = AutoModelForSequenceClassification.from_pretrained(goemotions_path).to(device)\n",
    "goemotions_tokenizer = AutoTokenizer.from_pretrained(goemotions_path)\n",
    "\n",
    "distress_model = AutoModelForSequenceClassification.from_pretrained(distress_path).to(device)\n",
    "distress_tokenizer = AutoTokenizer.from_pretrained(distress_path)\n",
    "\n",
    "# 2. Define emotion label mapping\n",
    "id2emotion = {\n",
    "    0: \"amusement\", 1: \"anger\", 2: \"annoyance\", 3: \"approval\", 4: \"caring\", 5: \"confusion\", 6: \"curiosity\",\n",
    "    7: \"desire\", 8: \"disappointment\", 9: \"disapproval\", 10: \"disgust\", 11: \"embarrassment\", 12: \"excitement\",\n",
    "    13: \"fear\", 14: \"gratitude\", 15: \"grief\", 16: \"joy\", 17: \"love\", 18: \"nervousness\", 19: \"neutral\",\n",
    "    20: \"optimism\", 21: \"pride\", 22: \"realization\", 23: \"relief\", 24: \"remorse\", 25: \"sadness\", 26: \"surprise\"\n",
    "}\n",
    "\n",
    "# Optional: Categorize emotions\n",
    "distress_emotions = {\"anger\", \"fear\", \"grief\", \"remorse\", \"sadness\", \"disappointment\", \"nervousness\"}\n",
    "non_distress_emotions = {\"joy\", \"love\", \"gratitude\", \"optimism\", \"amusement\", \"excitement\", \"surprise\"}\n",
    "\n",
    "# 3. Prediction function\n",
    "def hybrid_predict(text):\n",
    "    # Distress model\n",
    "    dist_input = distress_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        dist_output = distress_model(**dist_input)\n",
    "        is_distress = torch.argmax(dist_output.logits, dim=1).item()\n",
    "\n",
    "    # GoEmotions model\n",
    "    goemo_input = goemotions_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        goemo_output = goemotions_model(**goemo_input)\n",
    "        logits = goemo_output.logits\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    # Top predicted emotions\n",
    "    top_indices = np.argsort(probs)[::-1][:5]\n",
    "    top_emotions = [(id2emotion[i], round(probs[i], 4)) for i in top_indices]\n",
    "\n",
    "    # Filtered emotions\n",
    "    if is_distress:\n",
    "        filtered = [(emo, prob) for emo, prob in top_emotions if emo in distress_emotions]\n",
    "    else:\n",
    "        filtered = [(emo, prob) for emo, prob in top_emotions if emo in non_distress_emotions]\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"distress\": bool(is_distress),\n",
    "        \"top_emotions\": top_emotions,\n",
    "        \"filtered_emotions\": filtered\n",
    "    }\n",
    "\n",
    "# 4. Try on sample text\n",
    "sample = \"I feel so hopeless and tired of everything.\"\n",
    "result = hybrid_predict(sample)\n",
    "\n",
    "print(\"\\n Input Text:\")\n",
    "print(result[\"text\"])\n",
    "print(\"\\n Distress Detected:\" if result[\"distress\"] else \"\\n No Distress Detected:\")\n",
    "print(\"\\nTop 5 Emotions:\", result[\"top_emotions\"])\n",
    "print(\"Filtered Emotions:\", result[\"filtered_emotions\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sonicaid)",
   "language": "python",
   "name": "sonicaid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
